{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVAPtMDTYvRD"
   },
   "source": [
    "# Feature extraction: extraction of delta and theta frequency bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcABdbTciWqA"
   },
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1677517783528,
     "user": {
      "displayName": "Anna Grabowska",
      "userId": "15734344613010880864"
     },
     "user_tz": -60
    },
    "id": "N_cSZrE3hRG1"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import mne\n",
    "import copy\n",
    "import glob\n",
    "import array\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from autoreject import AutoReject\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from mne import Epochs, pick_types, events_from_annotations\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "from mne.preprocessing import Xdawn\n",
    "from mne.decoding import Vectorizer\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# parameters for plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constatnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "signal_frequency = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDt-UDhFqu0g"
   },
   "source": [
    "## Load EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    dir_path,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "    task = 'FLA',\n",
    "    reject_by_annotation=True\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    header_files_glob = os.path.join(os.path.abspath(\"\"), dir_path + \"*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        \n",
    "        try:\n",
    "            participant_epochs = load_epochs_from_file(file, task=task, reject_by_annotation=reject_by_annotation)\n",
    "\n",
    "            # and compute participant's id from file_name\n",
    "            participant_id = re.match(r\".*-(\\d+)_.*\", file).group(1)\n",
    "\n",
    "            f_bad = participant_epochs[\"f_bad\"].get_data()\n",
    "            f_good = participant_epochs[\"f_good\"].get_data()\n",
    "\n",
    "            # exclude those participants who have too few samples\n",
    "            if len(f_bad) < 5 or len(f_good) < 5:\n",
    "                # not enough data for this participant\n",
    "                continue\n",
    "\n",
    "            # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "            participant_df = create_df_from_epochs(\n",
    "                participant_id, participant_epochs, info_filename, info\n",
    "            )\n",
    "            print(participant_id)\n",
    "            go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "        \n",
    "        except:\n",
    "            print(\"No matching events found for f_good (event id 0)\")\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, participant_epochs, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename, dtype={'Demo_kod': object})\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"Demo_kod\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"Demo_kod\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "        \n",
    "    epoch_df = pd.DataFrame({\"id\": [id], \"epochs\": [participant_epochs]}).join(\n",
    "            info_df\n",
    "        )\n",
    "    participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_events(raw, task):\n",
    "    merged_events = None\n",
    "    merged_event_dict = None\n",
    "    \n",
    "    if task == 'MID':\n",
    "        event_dict = {\n",
    "            'Stimulus/F_BAD____*ex*incentive*FB': 10001,\n",
    "            'Stimulus/F_GOOD___*ex*incentive*FG': 10002,\n",
    "        }\n",
    "\n",
    "        # Map for merged correct/error response markers\n",
    "        merged_event_dict = {\"f_good\": 0, \"f_bad\": 1}\n",
    "\n",
    "        # Reconstruct the original events from Raw object\n",
    "        events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "        # Merge correct/error response events\n",
    "        merged_events = mne.merge_events(\n",
    "            events,\n",
    "            [10002],\n",
    "            merged_event_dict[\"f_good\"],\n",
    "            replace_events=True,\n",
    "        )\n",
    "\n",
    "        merged_events = mne.merge_events(\n",
    "            merged_events,\n",
    "            [10001],\n",
    "            merged_event_dict[\"f_bad\"],\n",
    "            replace_events=True,\n",
    "        )\n",
    "    elif task == 'GNG':\n",
    "        event_dict = {\n",
    "            'Stimulus/FB*ex*1_n*1_c_1*R': 10001,\n",
    "            'Stimulus/FB*ex*2_n*2_c_1*R': 10002,\n",
    "            'Stimulus/FG*ex*1_n*1_c_1*R': 10003,\n",
    "            'Stimulus/FG*ex*2_n*2_c_1*R': 10004,\n",
    "        }\n",
    "\n",
    "        # Map for merged correct/error response markers\n",
    "        merged_event_dict = {\"f_good\": 0, \"f_bad\": 1}\n",
    "\n",
    "        # Reconstruct the original events from Raw object\n",
    "        events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "        # Merge correct/error response events\n",
    "        merged_events = mne.merge_events(\n",
    "            events,\n",
    "            [10003, 10004],\n",
    "            merged_event_dict[\"f_good\"],\n",
    "            replace_events=True,\n",
    "        )\n",
    "\n",
    "        merged_events = mne.merge_events(\n",
    "            merged_events,\n",
    "            [10001, 10002],\n",
    "            merged_event_dict[\"f_bad\"],\n",
    "            replace_events=True,\n",
    "        )\n",
    "        \n",
    "    elif task == 'FLA':\n",
    "        event_dict = {\n",
    "            'Stimulus/F_BAD___*ex**lll*l*FB': 10001,\n",
    "            'Stimulus/F_BAD___*ex**lrl*r*FB': 10002,\n",
    "            'Stimulus/F_BAD___*ex**rlr*l*FB': 10003,\n",
    "            'Stimulus/F_BAD___*ex**rrr*r*FB': 10004,\n",
    "            'Stimulus/F_GOOD__*ex**lll*l*FG': 10005,\n",
    "            'Stimulus/F_GOOD__*ex**lrl*r*FG': 10006,\n",
    "            'Stimulus/F_GOOD__*ex**rlr*l*FG': 10007,\n",
    "            'Stimulus/F_GOOD__*ex**rrr*r*FG': 10008,\n",
    "        }\n",
    "\n",
    "        # Map for merged correct/error response markers\n",
    "        merged_event_dict = {\"f_good\": 0, \"f_bad\": 1}\n",
    "\n",
    "        # Reconstruct the original events from Raw object\n",
    "        events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "        # Merge correct/error response events\n",
    "        merged_events = mne.merge_events(\n",
    "            events,\n",
    "            [10005, 10006, 10007, 10008],\n",
    "            merged_event_dict[\"f_good\"],\n",
    "            replace_events=True,\n",
    "        )\n",
    "\n",
    "        merged_events = mne.merge_events(\n",
    "            merged_events,\n",
    "            [10001, 10002, 10003, 10004],\n",
    "            merged_event_dict[\"f_bad\"],\n",
    "            replace_events=True,\n",
    "        )\n",
    "    return merged_events, merged_event_dict      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, task, reject_by_annotation=True, mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file, eog=('HEOG', 'VEOG'))\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only GNG\n",
    "    merged_events, merged_event_dict = get_merged_events(raw, task)\n",
    "    \n",
    "    tmin=None\n",
    "    tmax=None\n",
    "    \n",
    "    if task == 'FLA' or task == 'GNG':\n",
    "        tmin, tmax = -1.0, 2.4  # Start and end of the segments\n",
    "    elif task == 'MID':\n",
    "        tmin, tmax = -0.2, 0.8  # Start and end of the segments\n",
    "\n",
    "# Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=reject_by_annotation,\n",
    "        preload=True,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    if not reject_by_annotation:\n",
    "        ar = AutoReject(random_state=random_state, n_jobs=10, verbose=0)\n",
    "        epochs_ar, reject_log = ar.fit_transform(epochs, return_log=True)\n",
    "        epochs = epochs_ar        \n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the data to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_autoreject_data = True\n",
    "task = 'GNG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "info_filename = 'data/sonata_data/questionnaires.csv'\n",
    "\n",
    "if load_autoreject_data:\n",
    "    df_name = f'sonata_data_{task}_autoreject'\n",
    "    dir_name = f'data/sonata_data/feedback_export_{task}/'\n",
    "    pickled_data_filename = 'data/sonata_data/' + df_name + '.pkl'\n",
    "\n",
    "    if os.path.isfile(pickled_data_filename):\n",
    "        print(\"Pickled file found. Loading pickled data...\")\n",
    "        data_df = pd.read_pickle(pickled_data_filename)\n",
    "        data_df.name = df_name\n",
    "        print(\"Done\")\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Pickled file not found. Loading data...\")\n",
    "        data_df = create_df_data(\n",
    "            dir_path=dir_name, \n",
    "            info=\"all\", \n",
    "            personal=False, \n",
    "            info_filename=info_filename, \n",
    "            task=task,\n",
    "            reject_by_annotation = False\n",
    "        )\n",
    "        \n",
    "        data_df.name = df_name\n",
    "        # save loaded data into a pickle file\n",
    "        data_df.to_pickle(\"data/sonata_data/\" + data_df.name + \".pkl\")\n",
    "        print(\"Done. Pickle file created\")\n",
    "else:\n",
    "    \n",
    "    df_name = f'sonata_data_{task}'\n",
    "    dir_name = f'data/sonata_data/feedback_export_{task}/'\n",
    "    pickled_data_filename = 'data/sonata_data/' + df_name + '.pkl'\n",
    "\n",
    "    if os.path.isfile(pickled_data_filename):\n",
    "        print(\"Pickled file found. Loading pickled data...\")\n",
    "        data_df = pd.read_pickle(pickled_data_filename)\n",
    "        data_df.name = df_name\n",
    "        print(\"Done\")\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Pickled file not found. Loading data...\")\n",
    "        data_df = create_df_data(\n",
    "            dir_path=dir_name, \n",
    "            info=\"all\", \n",
    "            personal=False, \n",
    "            info_filename=info_filename, \n",
    "            task=task,\n",
    "            reject_by_annotation = True\n",
    "        )\n",
    "        data_df.name = df_name\n",
    "        # save loaded data into a pickle file\n",
    "        data_df.to_pickle(\"data/sonata_data/\" + data_df.name + \".pkl\")\n",
    "        print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add frequency data: delta and theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ranges = [(0.1,3.0), (4.0,7.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "data_df['delta'] = data_df['epochs'].apply(lambda x: x.copy().filter(\n",
    "    None, \n",
    "    freq_ranges[0][1], \n",
    "    fir_design='firwin', \n",
    "    skip_by_annotation='edge',\n",
    "    h_trans_bandwidth = 'auto',\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "data_df['theta'] = data_df['epochs'].apply(lambda x: x.copy().filter(\n",
    "    freq_ranges[1][0], \n",
    "    freq_ranges[1][1], \n",
    "    fir_design='firwin', \n",
    "    skip_by_annotation='edge',\n",
    "    h_trans_bandwidth = 'auto',\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_pickle(\"data/sonata_data/\" + data_df.name + \"_freq.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define experimental and control groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data with frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle('data/sonata_data/sonata_data_GNG_autoreject_freq_short.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1677518482887,
     "user": {
      "displayName": "Anna Grabowska",
      "userId": "15734344613010880864"
     },
     "user_tz": -60
    },
    "id": "99n0TtUK9ksT",
    "outputId": "f3ba7a7e-1df9-40f7-8cb7-531dc5568a22"
   },
   "outputs": [],
   "source": [
    "dep = data_df[(data_df['BDI'] > 13) & (data_df['STAI'] > 41)]\n",
    "ctrl_dep = data_df[(data_df['BDI'] <= 13) & (data_df['STAI'] > 41)]\n",
    "anx = data_df[(data_df['BDI'] <= 13) & (data_df['STAI'] > 42)]\n",
    "ctrl_anx = data_df[(data_df['BDI'] <= 13) & (data_df['STAI'] < 41)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of groups stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_add_demo_info(demo_file_name, df):\n",
    "    demo_df = pd.read_csv(f'data/sonata_data/{demo_file_name}', dtype={'Demo_kod': object}, usecols=[\n",
    "        'Demo_kod',\n",
    "        'Płeć', \n",
    "        'Ręczność',\n",
    "        'Wiek',\n",
    "        'Wzrost',\n",
    "        'Waga',\n",
    "        'Twoja dotychczasowa liczba lat edukacji (w pełnych latach)'\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    demo_df.set_index('Demo_kod', inplace=True)\n",
    "    \n",
    "    group_indexes = df['Demo_kod'].to_list()\n",
    "    demo_ = demo_df.loc[group_indexes]\n",
    "        \n",
    "    df_indexed = df.set_index('Demo_kod')\n",
    "    \n",
    "    df_demo = pd.concat([df_indexed,demo_], axis=1)\n",
    "        \n",
    "    df_demo.rename(columns = {\n",
    "        'Płeć':'Sex',\n",
    "        'Wiek': 'Age',\n",
    "        'Ręczność': 'Handedness',\n",
    "        'Wzrost': 'Height',\n",
    "        'Waga': 'Weight',\n",
    "        'Twoja dotychczasowa liczba lat edukacji (w pełnych latach)':'Education (years)'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df_demo['Sex'] = df_demo['Sex'].astype(float)\n",
    "    \n",
    "    return df_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Full sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Płeć</th>\n",
       "      <th>Ręczność</th>\n",
       "      <th>Wiek</th>\n",
       "      <th>Wzrost</th>\n",
       "      <th>Waga</th>\n",
       "      <th>Twoja dotychczasowa liczba lat edukacji (w pełnych latach)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>225.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.495556</td>\n",
       "      <td>0.897778</td>\n",
       "      <td>23.640000</td>\n",
       "      <td>173.153333</td>\n",
       "      <td>68.186222</td>\n",
       "      <td>15.413333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499980</td>\n",
       "      <td>0.303616</td>\n",
       "      <td>4.176807</td>\n",
       "      <td>9.633220</td>\n",
       "      <td>14.820595</td>\n",
       "      <td>2.400298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Płeć    Ręczność        Wiek      Wzrost        Waga  \\\n",
       "count  225.000000  225.000000  225.000000  225.000000  225.000000   \n",
       "mean     0.495556    0.897778   23.640000  173.153333   68.186222   \n",
       "std      0.499980    0.303616    4.176807    9.633220   14.820595   \n",
       "min      0.000000    0.000000   18.000000  152.000000   20.000000   \n",
       "25%      0.000000    1.000000   21.000000  165.000000   58.000000   \n",
       "50%      0.000000    1.000000   23.000000  173.000000   66.000000   \n",
       "75%      1.000000    1.000000   25.000000  180.000000   76.000000   \n",
       "max      1.000000    1.000000   39.000000  200.000000  142.000000   \n",
       "\n",
       "       Twoja dotychczasowa liczba lat edukacji (w pełnych latach)  \n",
       "count                                         225.000000           \n",
       "mean                                           15.413333           \n",
       "std                                             2.400298           \n",
       "min                                             9.000000           \n",
       "25%                                            14.000000           \n",
       "50%                                            15.000000           \n",
       "75%                                            17.000000           \n",
       "max                                            25.000000           "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df = pd.read_csv(f'data/sonata_data/Sonata_scales.csv', dtype={'Demo_kod': object}, usecols=[\n",
    "        'Demo_kod',\n",
    "        'Płeć', \n",
    "        'Ręczność',\n",
    "        'Wiek',\n",
    "        'Wzrost',\n",
    "        'Waga',\n",
    "        'Twoja dotychczasowa liczba lat edukacji (w pełnych latach)'\n",
    "        ]\n",
    ")\n",
    "\n",
    "demo_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0.0, 1.0, 0.5])\n",
      "dict_values([113, 111, 1])\n"
     ]
    }
   ],
   "source": [
    "sex = demo_df['Płeć'].to_list()\n",
    "\n",
    "print(Counter(sex).keys()) # equals to list(set(words))\n",
    "print(Counter(sex).values()) # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sample after artifact rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BDI</th>\n",
       "      <th>BDI_cognitive</th>\n",
       "      <th>BDI_affective</th>\n",
       "      <th>BDI_affect_cog</th>\n",
       "      <th>BDI_somatic</th>\n",
       "      <th>STAI</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Handedness</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Education (years)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.959459</td>\n",
       "      <td>0.514324</td>\n",
       "      <td>0.499324</td>\n",
       "      <td>1.014550</td>\n",
       "      <td>0.611171</td>\n",
       "      <td>45.567568</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.896396</td>\n",
       "      <td>23.558559</td>\n",
       "      <td>173.186937</td>\n",
       "      <td>68.062613</td>\n",
       "      <td>15.409910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.231845</td>\n",
       "      <td>0.549190</td>\n",
       "      <td>0.544566</td>\n",
       "      <td>0.995998</td>\n",
       "      <td>0.503103</td>\n",
       "      <td>9.136097</td>\n",
       "      <td>0.499954</td>\n",
       "      <td>0.305434</td>\n",
       "      <td>4.096538</td>\n",
       "      <td>9.569345</td>\n",
       "      <td>14.759695</td>\n",
       "      <td>2.408223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>165.250000</td>\n",
       "      <td>57.250000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.750000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.830000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>5.170000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              BDI  BDI_cognitive  BDI_affective  BDI_affect_cog  BDI_somatic  \\\n",
       "count  222.000000     222.000000     222.000000      222.000000   222.000000   \n",
       "mean    11.959459       0.514324       0.499324        1.014550     0.611171   \n",
       "std     10.231845       0.549190       0.544566        0.995998     0.503103   \n",
       "min      0.000000       0.000000       0.000000        0.000000     0.000000   \n",
       "25%      5.000000       0.170000       0.000000        0.210000     0.170000   \n",
       "50%      9.000000       0.330000       0.330000        0.670000     0.500000   \n",
       "75%     16.750000       0.830000       0.670000        1.500000     0.830000   \n",
       "max     48.000000       2.830000       2.330000        5.170000     2.330000   \n",
       "\n",
       "             STAI         Sex  Handedness         Age      Height      Weight  \\\n",
       "count  222.000000  222.000000  222.000000  222.000000  222.000000  222.000000   \n",
       "mean    45.567568    0.493243    0.896396   23.558559  173.186937   68.062613   \n",
       "std      9.136097    0.499954    0.305434    4.096538    9.569345   14.759695   \n",
       "min     25.000000    0.000000    0.000000   18.000000  152.000000   20.000000   \n",
       "25%     39.000000    0.000000    1.000000   21.000000  165.250000   57.250000   \n",
       "50%     45.000000    0.000000    1.000000   23.000000  173.000000   66.000000   \n",
       "75%     52.000000    1.000000    1.000000   25.000000  180.000000   76.000000   \n",
       "max     70.000000    1.000000    1.000000   39.000000  200.000000  142.000000   \n",
       "\n",
       "       Education (years)  \n",
       "count         222.000000  \n",
       "mean           15.409910  \n",
       "std             2.408223  \n",
       "min             9.000000  \n",
       "25%            14.000000  \n",
       "50%            15.000000  \n",
       "75%            17.000000  \n",
       "max            25.000000  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_demo_df = df_add_demo_info(demo_file_name = 'Sonata_scales.csv', df=data_df)\n",
    "data_full_demo_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0.0, 1.0, 0.5])\n",
      "dict_values([112, 109, 1])\n"
     ]
    }
   ],
   "source": [
    "sex = data_full_demo_df['Sex'].to_list()\n",
    "\n",
    "print(Counter(sex).keys()) # equals to list(set(words))\n",
    "print(Counter(sex).values()) # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BDI</th>\n",
       "      <th>BDI_cognitive</th>\n",
       "      <th>BDI_affective</th>\n",
       "      <th>BDI_affect_cog</th>\n",
       "      <th>BDI_somatic</th>\n",
       "      <th>STAI</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Handedness</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Education (years)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.466667</td>\n",
       "      <td>1.053333</td>\n",
       "      <td>0.977067</td>\n",
       "      <td>2.031067</td>\n",
       "      <td>1.093467</td>\n",
       "      <td>54.106667</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>23.080000</td>\n",
       "      <td>171.133333</td>\n",
       "      <td>64.980000</td>\n",
       "      <td>15.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.855191</td>\n",
       "      <td>0.570844</td>\n",
       "      <td>0.588644</td>\n",
       "      <td>0.993065</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>6.235412</td>\n",
       "      <td>0.495748</td>\n",
       "      <td>0.310768</td>\n",
       "      <td>3.451439</td>\n",
       "      <td>9.719489</td>\n",
       "      <td>12.538169</td>\n",
       "      <td>2.372895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>163.500000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.830000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>5.170000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BDI  BDI_cognitive  BDI_affective  BDI_affect_cog  BDI_somatic  \\\n",
       "count  75.000000      75.000000      75.000000       75.000000    75.000000   \n",
       "mean   23.466667       1.053333       0.977067        2.031067     1.093467   \n",
       "std     8.855191       0.570844       0.588644        0.993065     0.474545   \n",
       "min    14.000000       0.000000       0.000000        0.670000     0.330000   \n",
       "25%    16.000000       0.670000       0.330000        1.330000     0.830000   \n",
       "50%    21.000000       1.000000       1.000000        1.830000     1.000000   \n",
       "75%    28.500000       1.170000       1.330000        2.500000     1.330000   \n",
       "max    48.000000       2.830000       2.330000        5.170000     2.330000   \n",
       "\n",
       "            STAI        Sex  Handedness        Age      Height     Weight  \\\n",
       "count  75.000000  75.000000   75.000000  75.000000   75.000000  75.000000   \n",
       "mean   54.106667   0.413333    0.893333  23.080000  171.133333  64.980000   \n",
       "std     6.235412   0.495748    0.310768   3.451439    9.719489  12.538169   \n",
       "min    42.000000   0.000000    0.000000  18.000000  155.000000  45.000000   \n",
       "25%    50.000000   0.000000    1.000000  21.000000  163.500000  55.000000   \n",
       "50%    55.000000   0.000000    1.000000  23.000000  169.000000  64.000000   \n",
       "75%    58.000000   1.000000    1.000000  25.000000  177.500000  71.000000   \n",
       "max    70.000000   1.000000    1.000000  38.000000  198.000000  97.000000   \n",
       "\n",
       "       Education (years)  \n",
       "count          75.000000  \n",
       "mean           15.466667  \n",
       "std             2.372895  \n",
       "min            11.000000  \n",
       "25%            14.000000  \n",
       "50%            15.000000  \n",
       "75%            17.000000  \n",
       "max            25.000000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_demo = df_add_demo_info(demo_file_name = 'Sonata_scales.csv', df=dep)\n",
    "dep_demo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([1.0, 0.0])\n",
      "dict_values([31, 44])\n"
     ]
    }
   ],
   "source": [
    "sex = dep_demo['Sex'].to_list()\n",
    "\n",
    "print(Counter(sex).keys()) # equals to list(set(words))\n",
    "print(Counter(sex).values()) # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Control for depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BDI</th>\n",
       "      <th>BDI_cognitive</th>\n",
       "      <th>BDI_affective</th>\n",
       "      <th>BDI_affect_cog</th>\n",
       "      <th>BDI_somatic</th>\n",
       "      <th>STAI</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Handedness</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Education (years)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.00000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.069444</td>\n",
       "      <td>0.310139</td>\n",
       "      <td>0.300278</td>\n",
       "      <td>0.611667</td>\n",
       "      <td>0.389028</td>\n",
       "      <td>47.194444</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>23.361111</td>\n",
       "      <td>172.381944</td>\n",
       "      <td>67.402778</td>\n",
       "      <td>15.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.678358</td>\n",
       "      <td>0.271451</td>\n",
       "      <td>0.318044</td>\n",
       "      <td>0.426902</td>\n",
       "      <td>0.294351</td>\n",
       "      <td>3.550992</td>\n",
       "      <td>0.496023</td>\n",
       "      <td>0.33304</td>\n",
       "      <td>4.280008</td>\n",
       "      <td>8.794802</td>\n",
       "      <td>17.550458</td>\n",
       "      <td>2.075266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>56.750000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>179.250000</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>16.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.670000</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BDI  BDI_cognitive  BDI_affective  BDI_affect_cog  BDI_somatic  \\\n",
       "count  72.000000      72.000000      72.000000       72.000000    72.000000   \n",
       "mean    7.069444       0.310139       0.300278        0.611667     0.389028   \n",
       "std     3.678358       0.271451       0.318044        0.426902     0.294351   \n",
       "min     0.000000       0.000000       0.000000        0.000000     0.000000   \n",
       "25%     4.000000       0.000000       0.000000        0.290000     0.170000   \n",
       "50%     7.000000       0.330000       0.330000        0.670000     0.330000   \n",
       "75%    11.000000       0.500000       0.330000        0.830000     0.500000   \n",
       "max    13.000000       1.170000       1.670000        1.830000     1.500000   \n",
       "\n",
       "            STAI        Sex  Handedness        Age      Height      Weight  \\\n",
       "count  72.000000  72.000000    72.00000  72.000000   72.000000   72.000000   \n",
       "mean   47.194444   0.437500     0.87500  23.361111  172.381944   67.402778   \n",
       "std     3.550992   0.496023     0.33304   4.280008    8.794802   17.550458   \n",
       "min    42.000000   0.000000     0.00000  18.000000  155.000000   20.000000   \n",
       "25%    45.000000   0.000000     1.00000  20.000000  165.000000   56.750000   \n",
       "50%    47.000000   0.000000     1.00000  22.000000  172.500000   63.500000   \n",
       "75%    50.000000   1.000000     1.00000  25.000000  179.250000   76.500000   \n",
       "max    55.000000   1.000000     1.00000  39.000000  190.000000  142.000000   \n",
       "\n",
       "       Education (years)  \n",
       "count          72.000000  \n",
       "mean           15.055556  \n",
       "std             2.075266  \n",
       "min             9.000000  \n",
       "25%            14.000000  \n",
       "50%            15.000000  \n",
       "75%            16.250000  \n",
       "max            20.000000  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_ctrl_demo = df_add_demo_info(demo_file_name = 'Sonata_scales.csv', df=ctrl_dep)\n",
    "dep_ctrl_demo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0.0, 1.0, 0.5])\n",
      "dict_values([40, 31, 1])\n"
     ]
    }
   ],
   "source": [
    "sex = dep_ctrl_demo['Sex'].to_list()\n",
    "\n",
    "print(Counter(sex).keys()) # equals to list(set(words))\n",
    "print(Counter(sex).values()) # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Anxiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BDI</th>\n",
       "      <th>BDI_cognitive</th>\n",
       "      <th>BDI_affective</th>\n",
       "      <th>BDI_affect_cog</th>\n",
       "      <th>BDI_somatic</th>\n",
       "      <th>STAI</th>\n",
       "      <th>Handedness</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Education (years)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.015385</td>\n",
       "      <td>0.310308</td>\n",
       "      <td>0.301846</td>\n",
       "      <td>0.613231</td>\n",
       "      <td>0.377231</td>\n",
       "      <td>47.753846</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>23.430769</td>\n",
       "      <td>172.576923</td>\n",
       "      <td>67.615385</td>\n",
       "      <td>15.046154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.676328</td>\n",
       "      <td>0.263106</td>\n",
       "      <td>0.322005</td>\n",
       "      <td>0.439961</td>\n",
       "      <td>0.293260</td>\n",
       "      <td>3.274097</td>\n",
       "      <td>0.348072</td>\n",
       "      <td>4.264568</td>\n",
       "      <td>8.700409</td>\n",
       "      <td>18.050011</td>\n",
       "      <td>2.139150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.670000</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BDI  BDI_cognitive  BDI_affective  BDI_affect_cog  BDI_somatic  \\\n",
       "count  65.000000      65.000000      65.000000       65.000000    65.000000   \n",
       "mean    7.015385       0.310308       0.301846        0.613231     0.377231   \n",
       "std     3.676328       0.263106       0.322005        0.439961     0.293260   \n",
       "min     0.000000       0.000000       0.000000        0.000000     0.000000   \n",
       "25%     4.000000       0.170000       0.000000        0.170000     0.170000   \n",
       "50%     7.000000       0.330000       0.330000        0.670000     0.330000   \n",
       "75%    10.000000       0.500000       0.330000        0.830000     0.500000   \n",
       "max    13.000000       1.170000       1.670000        1.830000     1.500000   \n",
       "\n",
       "            STAI  Handedness        Age      Height      Weight  \\\n",
       "count  65.000000   65.000000  65.000000   65.000000   65.000000   \n",
       "mean   47.753846    0.861538  23.430769  172.576923   67.615385   \n",
       "std     3.274097    0.348072   4.264568    8.700409   18.050011   \n",
       "min    43.000000    0.000000  18.000000  157.000000   20.000000   \n",
       "25%    45.000000    1.000000  20.000000  165.000000   56.000000   \n",
       "50%    47.000000    1.000000  22.000000  173.000000   64.000000   \n",
       "75%    50.000000    1.000000  25.000000  180.000000   78.000000   \n",
       "max    55.000000    1.000000  39.000000  190.000000  142.000000   \n",
       "\n",
       "       Education (years)  \n",
       "count          65.000000  \n",
       "mean           15.046154  \n",
       "std             2.139150  \n",
       "min             9.000000  \n",
       "25%            14.000000  \n",
       "50%            15.000000  \n",
       "75%            17.000000  \n",
       "max            20.000000  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anx_demo = df_add_demo_info(demo_file_name = 'Sonata_scales.csv', df=anx)\n",
    "anx_demo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0', '1', 'Osoba niebinarna'])\n",
      "dict_values([36, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "sex = anx_demo['Sex'].to_list()\n",
    "\n",
    "print(Counter(sex).keys()) # equals to list(set(words))\n",
    "print(Counter(sex).values()) # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Control for anxiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BDI</th>\n",
       "      <th>BDI_cognitive</th>\n",
       "      <th>BDI_affective</th>\n",
       "      <th>BDI_affect_cog</th>\n",
       "      <th>BDI_somatic</th>\n",
       "      <th>STAI</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Handedness</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Education (years)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.439394</td>\n",
       "      <td>0.151818</td>\n",
       "      <td>0.181061</td>\n",
       "      <td>0.333788</td>\n",
       "      <td>0.305455</td>\n",
       "      <td>34.893939</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>24.393939</td>\n",
       "      <td>176.090909</td>\n",
       "      <td>71.833333</td>\n",
       "      <td>15.878788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.314902</td>\n",
       "      <td>0.203567</td>\n",
       "      <td>0.262433</td>\n",
       "      <td>0.394367</td>\n",
       "      <td>0.274351</td>\n",
       "      <td>4.210349</td>\n",
       "      <td>0.484732</td>\n",
       "      <td>0.289683</td>\n",
       "      <td>4.643867</td>\n",
       "      <td>9.530078</td>\n",
       "      <td>12.981150</td>\n",
       "      <td>2.792931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.627500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>182.750000</td>\n",
       "      <td>80.750000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BDI  BDI_cognitive  BDI_affective  BDI_affect_cog  BDI_somatic  \\\n",
       "count  66.000000      66.000000      66.000000       66.000000    66.000000   \n",
       "mean    4.439394       0.151818       0.181061        0.333788     0.305455   \n",
       "std     3.314902       0.203567       0.262433        0.394367     0.274351   \n",
       "min     0.000000       0.000000       0.000000        0.000000     0.000000   \n",
       "25%     1.250000       0.000000       0.000000        0.000000     0.170000   \n",
       "50%     4.500000       0.170000       0.000000        0.170000     0.330000   \n",
       "75%     6.750000       0.170000       0.330000        0.627500     0.500000   \n",
       "max    12.000000       1.000000       1.330000        1.500000     1.330000   \n",
       "\n",
       "            STAI        Sex  Handedness        Age      Height      Weight  \\\n",
       "count  66.000000  66.000000   66.000000  66.000000   66.000000   66.000000   \n",
       "mean   34.893939   0.636364    0.909091  24.393939  176.090909   71.833333   \n",
       "std     4.210349   0.484732    0.289683   4.643867    9.530078   12.981150   \n",
       "min    25.000000   0.000000    0.000000  19.000000  152.000000   45.000000   \n",
       "25%    32.000000   0.000000    1.000000  21.000000  170.000000   62.000000   \n",
       "50%    35.000000   1.000000    1.000000  23.000000  176.000000   71.500000   \n",
       "75%    38.750000   1.000000    1.000000  26.000000  182.750000   80.750000   \n",
       "max    40.000000   1.000000    1.000000  39.000000  200.000000  107.000000   \n",
       "\n",
       "       Education (years)  \n",
       "count          66.000000  \n",
       "mean           15.878788  \n",
       "std             2.792931  \n",
       "min            12.000000  \n",
       "25%            14.000000  \n",
       "50%            15.000000  \n",
       "75%            17.000000  \n",
       "max            24.000000  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anx_ctrl_demo = df_add_demo_info(demo_file_name = 'Sonata_scales.csv', df=ctrl_anx)\n",
    "anx_ctrl_demo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0.0, 1.0])\n",
      "dict_values([24, 42])\n"
     ]
    }
   ],
   "source": [
    "sex = anx_ctrl_demo['Sex'].to_list()\n",
    "\n",
    "print(Counter(sex).keys()) # equals to list(set(words))\n",
    "print(Counter(sex).values()) # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test differences in BDI and STAI scores between groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- STAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_depression_tai = ctrl_dep['STAI'].to_numpy()\n",
    "depression_tai = dep['STAI'].to_numpy()\n",
    "\n",
    "print(f\"Depression group: mean TAI score: {depression_tai.mean()} SD = {depression_tai.std()}\")\n",
    "print(f\"Control group: mean TAI score: {control_depression_tai.mean()} SD = {control_depression_tai.std()}\")\n",
    "\n",
    "t_value, p_value = ttest_rel(depression_tai[1:], control_depression_tai)\n",
    "print(f\"t({len(control_depression_tai) -1}) = {t_value}, p = {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_depression_bdi = ctrl_dep['BDI'].to_numpy()\n",
    "depression_bdi = dep['BDI'].to_numpy()\n",
    "\n",
    "print(f\"Depression group: mean BDI score: {depression_bdi.mean()} SD = {depression_bdi.std()}\")\n",
    "print(f\"Control group: mean BDI score: {control_depression_bdi.mean()} SD = {control_depression_bdi.std()}\")\n",
    "\n",
    "t_value, p_value = ttest_rel(depression_bdi[1:], control_depression_bdi)\n",
    "print(f\"t({len(depression_bdi[1:]) -1}) = {t_value}, p = {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- STAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_anx_tai = ctrl_anx['STAI'].to_numpy()\n",
    "anx_tai = anx['STAI'].to_numpy()\n",
    "\n",
    "print(f\"Anxiety group: mean TAI score: {anx_tai.mean()} SD = {anx_tai.std()}\")\n",
    "print(f\"Control group: mean TAI score: {control_anx_tai.mean()} SD = {control_anx_tai.std()}\")\n",
    "\n",
    "t_value, p_value = ttest_rel(anx_tai, control_anx_tai[2:])\n",
    "print(f\"t({len(anx_tai[1:]) -1}) = {t_value}, p = {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_anx_bdi = ctrl_anx['BDI'].to_numpy()\n",
    "anx_bdi = anx['BDI'].to_numpy()\n",
    "\n",
    "print(f\"Anxiety group: mean BDI score: {anx_bdi.mean()} SD = {anx_bdi.std()}\")\n",
    "print(f\"Control group: mean BDI score: {control_anx_bdi.mean()} SD = {control_anx_bdi.std()}\")\n",
    "\n",
    "t_value, p_value = ttest_rel(anx_bdi[1:], control_anx_bdi)\n",
    "print(f\"t({len(anx_bdi[1:]) -1}) = {t_value}, p = {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "74g2chqQWa48",
    "3irne7dXaoIt"
   ],
   "name": "",
   "provenance": [
    {
     "file_id": "13OESwog1Uwt1WAcNW9QEaphoYxPyNplr",
     "timestamp": 1677327376830
    }
   ],
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "bluish",
   "language": "python",
   "name": "bluish"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
